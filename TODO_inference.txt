
**** FOR CPU
USE_OPENMP=0 # default 1
USE_TBB=1
BLAS=MKL

= BLAS (Basic Linear Algebra Subprograms)
  = OpenBLAS - open-source BLAS
  = MKL (Intel Maths Kernel) - optimised numerical libraries including BLAS, LAPACK, and ScaLAPACK
  = cuBLAS - CUDA BLAS
= MKLDNN - open-source optimized Deep Neural Networks primitives. Can link with BLAS or MKL
  = cuDNN - CUDA DNN


**** GPTJForCausalLM
= Multi-Attention Heads (K, V, Q, ProjOut)
= MLP Multi Language Processing

n_head 16
n_layer 28 # hidden layers
n_embd 4096 # dim emb and hidden states (hidden_size = n_embd)
rotary_dim 64
vocab_size * embed_dim (num embeddings)

GPTJAttention
16M (q_proj):   Linear(in_features=4096, out_features=4096, bias=False)
16M (k_proj):   Linear(in_features=4096, out_features=4096, bias=False)
16M (v_proj):   Linear(in_features=4096, out_features=4096, bias=False)
16M (out_proj): Linear(in_features=4096, out_features=4096, bias=False)
~64M * f16 = 128MB

GPTJMLP
 8KB (ln_x):    LayerNorm(4096, bias=True)
128M (fc_in):   Linear(in_features=4096, out_features=16384, bias=True)
128M (fc_out):  Linear(in_features=16384, out_features=4096, bias=True)
256M * f16 = 512MB

GPTJModel
 8KB (ln_f):    LayerNorm(4096, bias=True)


200M (wte): Embedding(50400, 4096)
~400MB (200M*f16) besides layers

---- 11,273 MB ~11.01GB
GPTJModel(
  (wte): Embedding(50400, 4096)
  (drop): Dropout(p=0.0, inplace=False)
  (h): ModuleList(
    (0-27): 28 x GPTJBlock(
      (ln_1): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)
      (attn): GPTJAttention(
        (attn_dropout): Dropout(p=0.0, inplace=False)
        (resid_dropout): Dropout(p=0.0, inplace=False)
        (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
        (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
        (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
        (out_proj): Linear(in_features=4096, out_features=4096, bias=False)
      )
      (mlp): GPTJMLP(
        (fc_in): Linear(in_features=4096, out_features=16384, bias=True)
        (fc_out): Linear(in_features=16384, out_features=4096, bias=True)
        (act): NewGELUActivation()
        (dropout): Dropout(p=0.0, inplace=False)
      )
    )
  )
  (ln_f): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)
)